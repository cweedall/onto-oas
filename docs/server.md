# Generating the server

OBA uses [OpenAPI Generator](https://github.com/OpenAPITools/openapi-generator#overview) to generate the server.

## OpenAPI Generator

OpenAPI Generator allows generation of API client libraries (SDK generation), server stubs, documentation and 
configuration automatically given an OpenAPI Spec (both 2.0 and 3.0 are supported). The OpenAPI Generator website
maintains a list of languages/frameworks are supported.

### Integration with RDF

The OpenAPI Generator easily provide an API backend for mocking based on the OpenAPI Specification. However, it does not
provide the logic of the server (For example, a database connection).

OBA modifies the server generated by OpenAPI Generator using templates. As result, the server can:

 1. Connect and query resources from the SPARQL endpoint.
    - Convert the RDF triples to JSON (using JSON/LD).
 2. Connect and insert resources to the SPARQL endpoint.
    - Convert the JSON to RDF triples (using JSON/LD).
    
## Generating the server

Currently, the following languages/frameworks are supported by OBA:

- Python/flash

### How to generate it?

!!! warning
    You must install Docker.
    
Run the script.

```bash
$ bash generate-server.sh
...
...
SUCCESS
```

#### Structure of the server

The server directory contains the following files and directories

```bash
$ ls server/
Dockerfile: A Dockerfile to build the Docker Images
README.md: A README.md with the instructions to run the server
contexts: The directory with the JSON/LD contests
openapi_server: The server implemenation
queries: The directory with the SPARQL queries
requirements.txt: The Python requirements of the server 
test-requirements.txt: The Python requirements of testing the server 
```


### Generating the JSON/LD context

Since the adoption of JSON/LD is in progress, OBA can convert the format of the responses from JSON/LD to JSON.
To do that, OBA requires a file with the context. Simply speaking, a context is used to map terms to IRIs. 

!!! info 
    Visit the [JSON/LD documentation](https://json-ld.org/spec/latest/json-ld/#the-context) for more information

Currently, OBA does not support the generation of the context file. 
We recommend to use the project [owl2jsonld](https://github.com/sirspock/owl2jsonld)

!!! info
    This package is a fork of [owl2jsonld](https://github.com/stain/owl2jsonld). Thanks Stain!

```bash
wget https://github.com/sirspock/owl2jsonld/releases/download/0.3.0/owl2jsonld-0.3.0-SNAPSHOT-standalone.jar
java -jar owl2jsonld-0.3.0-SNAPSHOT-standalone.jar  \
    https://mintproject.github.io/Mint-ModelCatalog-Ontology/release/1.2.0/ontology.xml > server/context.json
```


#### One ontology

If you are using one only. You can run:

```bash
java -jar owl2jsonld-0.3.0-SNAPSHOT-standalone.jar  \
    https://mintproject.github.io/Mint-ModelCatalog-Ontology/release/1.2.0/ontology.xml > server/context.json
```

#### Two or more ontologies

To merge the multiple JSON files, you can install the [jq](https://stedolan.github.io/jq/)

```bash

java -jar owl2jsonld-0.3.0-SNAPSHOT-standalone.jar  \
    $ONTOLOGY_URL1 > a.json
java -jar owl2jsonld-0.3.0-SNAPSHOT-standalone.jar \
    $ONTOLOGY_URL2 > b.json    
jq -s '.[0] * .[1]' a.json b.json  | jq -S > server/contexts/context.json
rm a.json b.json

```

For example, the Model Catalog ontology:

```bash

java -jar owl2jsonld-0.3.0-SNAPSHOT-standalone.jar  \
    https://mintproject.github.io/Mint-ModelCatalog-Ontology/release/1.2.0/ontology.xml > a.json
java -jar owl2jsonld-0.3.0-SNAPSHOT-standalone.jar \
    https://knowledgecaptureanddiscovery.github.io/SoftwareDescriptionOntology/release/1.2.0/ontology.xml > b.json
jq -s '.[0] * .[1]' a.json b.json  | jq -S > server/contexts/context.json
rm a.json b.json

```

## Running the server

## Running with Docker

To run the server on a Docker container, please execute the following from the root directory:

```bash
# building the image
docker build -t openapi_server .

# starting up a container
docker run -p 8080:8080 openapi_server
```


and open your browser to here:
!!! warning
    The version (v1.3.0) depends of your configuration. If you have questions, go to the README.


```
http://localhost:8080/v1.3.0/ui/
```

Your OpenAPI definition lives here:

```
http://localhost:8080/v1.3.0/openapi.json
```

To launch the integration tests, use tox:
```
sudo pip install tox
tox
```

## How to use it?

